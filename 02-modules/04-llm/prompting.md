# Prompt Engineering

## Overview
- Prompt engineering refers to the process of designing and `refining prompts` or `instructions` provided to `Large Language Models(LLM)` in order to elicit desired responses.
- Prompt engineering is crucial for fine-tuning and controlling the behavior of language models.
- This requires a deep understanding of the language model's capabilities, limitations, and potential biases.

## Types of LLM
- With the bearing of Prompt Engineering, LLM's are categorized into two types:
  - **Base:**
    - It refers to the pre-trained model provided by the model's developers, such as GPT-3.5 or GPT-3.
    - It serves as the starting point for further fine-tuning and customization.
    - It is is trained on a large corpus of text data and has general knowledge about various topics and language patterns.
    - It predicts next word based on text training data.
  - **Instruction Tuned:**
    - It is flavoured base LLM that has undergone additional training and customization by incorporating specific instructions or prompts to achieve desired behaviors or outputs.
    - It fine tunes the instrution.
    - It uses Reinforcement Learning with Human Feedback to tune it responses.
    - These are often developed to address limitations or biases present in the base model, enhance performance in specific tasks, or adapt the model to domain-specific contexts.
- 